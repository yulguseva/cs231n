{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.334198\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.017035 analytic: 1.017035, relative error: 1.380184e-08\n",
      "numerical: 1.038697 analytic: 1.038697, relative error: 6.509477e-08\n",
      "numerical: -0.653765 analytic: -0.653765, relative error: 8.441382e-09\n",
      "numerical: 5.869926 analytic: 5.869925, relative error: 1.075586e-08\n",
      "numerical: -1.433655 analytic: -1.433655, relative error: 1.157443e-08\n",
      "numerical: 1.191432 analytic: 1.191432, relative error: 1.332974e-08\n",
      "numerical: -1.845151 analytic: -1.845151, relative error: 1.735911e-08\n",
      "numerical: 0.376688 analytic: 0.376688, relative error: 1.271455e-08\n",
      "numerical: 3.242388 analytic: 3.242387, relative error: 1.878507e-08\n",
      "numerical: 5.120279 analytic: 5.120279, relative error: 9.540713e-09\n",
      "numerical: 0.735915 analytic: 0.735915, relative error: 2.004230e-08\n",
      "numerical: -1.209475 analytic: -1.209475, relative error: 5.975009e-09\n",
      "numerical: -0.063073 analytic: -0.063073, relative error: 7.924653e-07\n",
      "numerical: -0.858441 analytic: -0.858441, relative error: 1.547170e-08\n",
      "numerical: 0.609218 analytic: 0.609218, relative error: 8.113056e-08\n",
      "numerical: 0.668853 analytic: 0.668853, relative error: 1.039398e-08\n",
      "numerical: -2.792112 analytic: -2.792112, relative error: 7.780008e-09\n",
      "numerical: -2.774727 analytic: -2.774727, relative error: 1.042305e-08\n",
      "numerical: 1.767665 analytic: 1.767665, relative error: 7.145296e-09\n",
      "numerical: -0.554294 analytic: -0.554294, relative error: 1.525617e-07\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.334198e+00 computed in 0.081135s\n",
      "vectorized loss: 2.334198e+00 computed in 0.005220s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 775.433576\n",
      "iteration 100 / 1500: loss 284.846817\n",
      "iteration 200 / 1500: loss 105.583433\n",
      "iteration 300 / 1500: loss 40.071420\n",
      "iteration 400 / 1500: loss 15.923129\n",
      "iteration 500 / 1500: loss 7.199200\n",
      "iteration 600 / 1500: loss 3.889563\n",
      "iteration 700 / 1500: loss 2.741101\n",
      "iteration 800 / 1500: loss 2.287128\n",
      "iteration 900 / 1500: loss 2.191206\n",
      "iteration 1000 / 1500: loss 2.062340\n",
      "iteration 1100 / 1500: loss 2.067994\n",
      "iteration 1200 / 1500: loss 2.060253\n",
      "iteration 1300 / 1500: loss 2.065747\n",
      "iteration 1400 / 1500: loss 2.058562\n",
      "That took 5.211335s\n"
     ]
    }
   ],
   "source": [
    "# In the file linear_classifier.py, implement SGD in the function\n",
    "# LinearClassifier.train() and then run it with the code below.\n",
    "from cs231n.classifiers import LinearSVM\n",
    "clf = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = clf.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
    "                      num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1e-07 25000\n",
      "iteration 0 / 2000: loss 765.812010\n",
      "iteration 100 / 2000: loss 281.278230\n",
      "iteration 200 / 2000: loss 104.171046\n",
      "iteration 300 / 2000: loss 39.496054\n",
      "iteration 400 / 2000: loss 15.674550\n",
      "iteration 500 / 2000: loss 7.092281\n",
      "iteration 600 / 2000: loss 3.856546\n",
      "iteration 700 / 2000: loss 2.750068\n",
      "iteration 800 / 2000: loss 2.358527\n",
      "iteration 900 / 2000: loss 2.163326\n",
      "iteration 1000 / 2000: loss 2.131727\n",
      "iteration 1100 / 2000: loss 2.131142\n",
      "iteration 1200 / 2000: loss 2.055900\n",
      "iteration 1300 / 2000: loss 2.125219\n",
      "iteration 1400 / 2000: loss 2.054094\n",
      "iteration 1500 / 2000: loss 2.085542\n",
      "iteration 1600 / 2000: loss 2.037756\n",
      "iteration 1700 / 2000: loss 2.164697\n",
      "iteration 1800 / 2000: loss 2.134030\n",
      "iteration 1900 / 2000: loss 2.087001\n",
      "training accuracy: 0.320449\n",
      "validation accuracy: 0.336000\n",
      "\n",
      " 1e-07 31250\n",
      "iteration 0 / 2000: loss 970.338308\n",
      "iteration 100 / 2000: loss 277.527591\n",
      "iteration 200 / 2000: loss 80.442712\n",
      "iteration 300 / 2000: loss 24.422707\n",
      "iteration 400 / 2000: loss 8.460715\n",
      "iteration 500 / 2000: loss 3.933560\n",
      "iteration 600 / 2000: loss 2.598985\n",
      "iteration 700 / 2000: loss 2.279526\n",
      "iteration 800 / 2000: loss 2.102231\n",
      "iteration 900 / 2000: loss 2.115062\n",
      "iteration 1000 / 2000: loss 2.097881\n",
      "iteration 1100 / 2000: loss 2.082050\n",
      "iteration 1200 / 2000: loss 2.103582\n",
      "iteration 1300 / 2000: loss 2.096549\n",
      "iteration 1400 / 2000: loss 2.109741\n",
      "iteration 1500 / 2000: loss 2.126964\n",
      "iteration 1600 / 2000: loss 2.196739\n",
      "iteration 1700 / 2000: loss 2.122031\n",
      "iteration 1800 / 2000: loss 2.144715\n",
      "iteration 1900 / 2000: loss 2.070130\n",
      "training accuracy: 0.323878\n",
      "validation accuracy: 0.336000\n",
      "\n",
      " 1e-07 37500\n",
      "iteration 0 / 2000: loss 1160.080968\n",
      "iteration 100 / 2000: loss 257.893797\n",
      "iteration 200 / 2000: loss 58.741334\n",
      "iteration 300 / 2000: loss 14.673208\n",
      "iteration 400 / 2000: loss 4.911115\n",
      "iteration 500 / 2000: loss 2.699144\n",
      "iteration 600 / 2000: loss 2.250954\n",
      "iteration 700 / 2000: loss 2.141686\n",
      "iteration 800 / 2000: loss 2.084298\n",
      "iteration 900 / 2000: loss 2.078329\n",
      "iteration 1000 / 2000: loss 2.108507\n",
      "iteration 1100 / 2000: loss 2.164127\n",
      "iteration 1200 / 2000: loss 2.135765\n",
      "iteration 1300 / 2000: loss 2.152131\n",
      "iteration 1400 / 2000: loss 2.109132\n",
      "iteration 1500 / 2000: loss 2.125668\n",
      "iteration 1600 / 2000: loss 2.124167\n",
      "iteration 1700 / 2000: loss 2.105462\n",
      "iteration 1800 / 2000: loss 2.138654\n",
      "iteration 1900 / 2000: loss 2.161967\n",
      "training accuracy: 0.309878\n",
      "validation accuracy: 0.324000\n",
      "\n",
      " 1e-07 43750\n",
      "iteration 0 / 2000: loss 1350.043379\n",
      "iteration 100 / 2000: loss 233.768443\n",
      "iteration 200 / 2000: loss 41.996595\n",
      "iteration 300 / 2000: loss 9.005501\n",
      "iteration 400 / 2000: loss 3.296902\n",
      "iteration 500 / 2000: loss 2.361447\n",
      "iteration 600 / 2000: loss 2.170521\n",
      "iteration 700 / 2000: loss 2.068186\n",
      "iteration 800 / 2000: loss 2.171895\n",
      "iteration 900 / 2000: loss 2.191830\n",
      "iteration 1000 / 2000: loss 2.082946\n",
      "iteration 1100 / 2000: loss 2.200876\n",
      "iteration 1200 / 2000: loss 2.149956\n",
      "iteration 1300 / 2000: loss 2.109964\n",
      "iteration 1400 / 2000: loss 2.123946\n",
      "iteration 1500 / 2000: loss 2.090130\n",
      "iteration 1600 / 2000: loss 2.135330\n",
      "iteration 1700 / 2000: loss 2.107765\n",
      "iteration 1800 / 2000: loss 2.107928\n",
      "iteration 1900 / 2000: loss 2.132697\n",
      "training accuracy: 0.316367\n",
      "validation accuracy: 0.322000\n",
      "\n",
      " 2e-07 25000\n",
      "iteration 0 / 2000: loss 773.544235\n",
      "iteration 100 / 2000: loss 104.602752\n",
      "iteration 200 / 2000: loss 15.718362\n",
      "iteration 300 / 2000: loss 3.944240\n",
      "iteration 400 / 2000: loss 2.302534\n",
      "iteration 500 / 2000: loss 2.046184\n",
      "iteration 600 / 2000: loss 2.060953\n",
      "iteration 700 / 2000: loss 2.066781\n",
      "iteration 800 / 2000: loss 2.109219\n",
      "iteration 900 / 2000: loss 2.082919\n",
      "iteration 1000 / 2000: loss 2.109025\n",
      "iteration 1100 / 2000: loss 2.097660\n",
      "iteration 1200 / 2000: loss 2.099963\n",
      "iteration 1300 / 2000: loss 2.062582\n",
      "iteration 1400 / 2000: loss 2.122977\n",
      "iteration 1500 / 2000: loss 2.100610\n",
      "iteration 1600 / 2000: loss 2.054857\n",
      "iteration 1700 / 2000: loss 2.045546\n",
      "iteration 1800 / 2000: loss 2.065792\n",
      "iteration 1900 / 2000: loss 2.061765\n",
      "training accuracy: 0.328592\n",
      "validation accuracy: 0.345000\n",
      "\n",
      " 2e-07 31250\n",
      "iteration 0 / 2000: loss 949.482975\n",
      "iteration 100 / 2000: loss 78.200946\n",
      "iteration 200 / 2000: loss 8.224680\n",
      "iteration 300 / 2000: loss 2.598346\n",
      "iteration 400 / 2000: loss 2.166226\n",
      "iteration 500 / 2000: loss 2.127785\n",
      "iteration 600 / 2000: loss 2.139451\n",
      "iteration 700 / 2000: loss 2.121790\n",
      "iteration 800 / 2000: loss 2.125868\n",
      "iteration 900 / 2000: loss 2.123772\n",
      "iteration 1000 / 2000: loss 2.034149\n",
      "iteration 1100 / 2000: loss 2.165429\n",
      "iteration 1200 / 2000: loss 2.075330\n",
      "iteration 1300 / 2000: loss 2.138719\n",
      "iteration 1400 / 2000: loss 2.111333\n",
      "iteration 1500 / 2000: loss 2.120813\n",
      "iteration 1600 / 2000: loss 2.046810\n",
      "iteration 1700 / 2000: loss 2.120732\n",
      "iteration 1800 / 2000: loss 2.129911\n",
      "iteration 1900 / 2000: loss 2.126114\n",
      "training accuracy: 0.324898\n",
      "validation accuracy: 0.340000\n",
      "\n",
      " 2e-07 37500\n",
      "iteration 0 / 2000: loss 1156.866555\n",
      "iteration 100 / 2000: loss 57.942582\n",
      "iteration 200 / 2000: loss 4.868672\n",
      "iteration 300 / 2000: loss 2.222297\n",
      "iteration 400 / 2000: loss 2.121683\n",
      "iteration 500 / 2000: loss 2.140820\n",
      "iteration 600 / 2000: loss 2.136796\n",
      "iteration 700 / 2000: loss 2.112768\n",
      "iteration 800 / 2000: loss 2.142230\n",
      "iteration 900 / 2000: loss 2.082780\n",
      "iteration 1000 / 2000: loss 2.085424\n",
      "iteration 1100 / 2000: loss 2.135225\n",
      "iteration 1200 / 2000: loss 2.130759\n",
      "iteration 1300 / 2000: loss 2.105586\n",
      "iteration 1400 / 2000: loss 2.145203\n",
      "iteration 1500 / 2000: loss 2.066247\n",
      "iteration 1600 / 2000: loss 2.128038\n",
      "iteration 1700 / 2000: loss 2.089406\n",
      "iteration 1800 / 2000: loss 2.152013\n",
      "iteration 1900 / 2000: loss 2.097488\n",
      "training accuracy: 0.320694\n",
      "validation accuracy: 0.327000\n",
      "\n",
      " 2e-07 43750\n",
      "iteration 0 / 2000: loss 1345.012793\n",
      "iteration 100 / 2000: loss 41.211509\n",
      "iteration 200 / 2000: loss 3.226519\n",
      "iteration 300 / 2000: loss 2.175757\n",
      "iteration 400 / 2000: loss 2.112980\n",
      "iteration 500 / 2000: loss 2.106888\n",
      "iteration 600 / 2000: loss 2.157696\n",
      "iteration 700 / 2000: loss 2.140548\n",
      "iteration 800 / 2000: loss 2.119498\n",
      "iteration 900 / 2000: loss 2.120719\n",
      "iteration 1000 / 2000: loss 2.221282\n",
      "iteration 1100 / 2000: loss 2.231964\n",
      "iteration 1200 / 2000: loss 2.151715\n",
      "iteration 1300 / 2000: loss 2.113159\n",
      "iteration 1400 / 2000: loss 2.135709\n",
      "iteration 1500 / 2000: loss 2.166206\n",
      "iteration 1600 / 2000: loss 2.064808\n",
      "iteration 1700 / 2000: loss 2.123277\n",
      "iteration 1800 / 2000: loss 2.162419\n",
      "iteration 1900 / 2000: loss 2.120801\n",
      "training accuracy: 0.299755\n",
      "validation accuracy: 0.321000\n",
      "\n",
      " 3e-07 25000\n",
      "iteration 0 / 2000: loss 772.210064\n",
      "iteration 100 / 2000: loss 39.181776\n",
      "iteration 200 / 2000: loss 3.890983\n",
      "iteration 300 / 2000: loss 2.126094\n",
      "iteration 400 / 2000: loss 2.107240\n",
      "iteration 500 / 2000: loss 2.079610\n",
      "iteration 600 / 2000: loss 2.086867\n",
      "iteration 700 / 2000: loss 2.019417\n",
      "iteration 800 / 2000: loss 2.083371\n",
      "iteration 900 / 2000: loss 2.053291\n",
      "iteration 1000 / 2000: loss 2.135475\n",
      "iteration 1100 / 2000: loss 2.102849\n",
      "iteration 1200 / 2000: loss 2.113276\n",
      "iteration 1300 / 2000: loss 2.134951\n",
      "iteration 1400 / 2000: loss 2.008123\n",
      "iteration 1500 / 2000: loss 2.058514\n",
      "iteration 1600 / 2000: loss 2.078146\n",
      "iteration 1700 / 2000: loss 2.109446\n",
      "iteration 1800 / 2000: loss 2.077429\n",
      "iteration 1900 / 2000: loss 2.110407\n",
      "training accuracy: 0.320776\n",
      "validation accuracy: 0.332000\n",
      "\n",
      " 3e-07 31250\n",
      "iteration 0 / 2000: loss 965.029468\n",
      "iteration 100 / 2000: loss 23.734983\n",
      "iteration 200 / 2000: loss 2.622661\n",
      "iteration 300 / 2000: loss 2.058953\n",
      "iteration 400 / 2000: loss 2.033772\n",
      "iteration 500 / 2000: loss 2.091100\n",
      "iteration 600 / 2000: loss 2.078075\n",
      "iteration 700 / 2000: loss 2.114224\n",
      "iteration 800 / 2000: loss 2.064080\n",
      "iteration 900 / 2000: loss 2.145355\n",
      "iteration 1000 / 2000: loss 2.080946\n",
      "iteration 1100 / 2000: loss 2.111469\n",
      "iteration 1200 / 2000: loss 2.110952\n",
      "iteration 1300 / 2000: loss 2.119449\n",
      "iteration 1400 / 2000: loss 2.125843\n",
      "iteration 1500 / 2000: loss 2.156726\n",
      "iteration 1600 / 2000: loss 2.090223\n",
      "iteration 1700 / 2000: loss 2.118686\n",
      "iteration 1800 / 2000: loss 2.111570\n",
      "iteration 1900 / 2000: loss 2.142130\n",
      "training accuracy: 0.320041\n",
      "validation accuracy: 0.334000\n",
      "\n",
      " 3e-07 37500\n",
      "iteration 0 / 2000: loss 1152.302103\n",
      "iteration 100 / 2000: loss 14.153577\n",
      "iteration 200 / 2000: loss 2.206685\n",
      "iteration 300 / 2000: loss 2.114743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 2000: loss 2.100591\n",
      "iteration 500 / 2000: loss 2.107757\n",
      "iteration 600 / 2000: loss 2.106759\n",
      "iteration 700 / 2000: loss 2.114722\n",
      "iteration 800 / 2000: loss 2.135085\n",
      "iteration 900 / 2000: loss 2.137658\n",
      "iteration 1000 / 2000: loss 2.126974\n",
      "iteration 1100 / 2000: loss 2.114236\n",
      "iteration 1200 / 2000: loss 2.097615\n",
      "iteration 1300 / 2000: loss 2.140414\n",
      "iteration 1400 / 2000: loss 2.115752\n",
      "iteration 1500 / 2000: loss 2.104648\n",
      "iteration 1600 / 2000: loss 2.090021\n",
      "iteration 1700 / 2000: loss 2.090658\n",
      "iteration 1800 / 2000: loss 2.103172\n",
      "iteration 1900 / 2000: loss 2.098151\n",
      "training accuracy: 0.315122\n",
      "validation accuracy: 0.332000\n",
      "\n",
      " 3e-07 43750\n",
      "iteration 0 / 2000: loss 1339.077168\n",
      "iteration 100 / 2000: loss 8.608927\n",
      "iteration 200 / 2000: loss 2.154929\n",
      "iteration 300 / 2000: loss 2.132857\n",
      "iteration 400 / 2000: loss 2.123675\n",
      "iteration 500 / 2000: loss 2.147888\n",
      "iteration 600 / 2000: loss 2.106150\n",
      "iteration 700 / 2000: loss 2.138455\n",
      "iteration 800 / 2000: loss 2.093954\n",
      "iteration 900 / 2000: loss 2.127446\n",
      "iteration 1000 / 2000: loss 2.128051\n",
      "iteration 1100 / 2000: loss 2.081770\n",
      "iteration 1200 / 2000: loss 2.135759\n",
      "iteration 1300 / 2000: loss 2.157811\n",
      "iteration 1400 / 2000: loss 2.121697\n",
      "iteration 1500 / 2000: loss 2.118839\n",
      "iteration 1600 / 2000: loss 2.133587\n",
      "iteration 1700 / 2000: loss 2.107380\n",
      "iteration 1800 / 2000: loss 2.128510\n",
      "iteration 1900 / 2000: loss 2.126787\n",
      "training accuracy: 0.308204\n",
      "validation accuracy: 0.311000\n",
      "\n",
      " 4e-07 25000\n",
      "iteration 0 / 2000: loss 761.556908\n",
      "iteration 100 / 2000: loss 15.238863\n",
      "iteration 200 / 2000: loss 2.331308\n",
      "iteration 300 / 2000: loss 2.098041\n",
      "iteration 400 / 2000: loss 2.073471\n",
      "iteration 500 / 2000: loss 2.136412\n",
      "iteration 600 / 2000: loss 2.049958\n",
      "iteration 700 / 2000: loss 2.101169\n",
      "iteration 800 / 2000: loss 2.064327\n",
      "iteration 900 / 2000: loss 2.065810\n",
      "iteration 1000 / 2000: loss 2.033552\n",
      "iteration 1100 / 2000: loss 2.034166\n",
      "iteration 1200 / 2000: loss 2.076579\n",
      "iteration 1300 / 2000: loss 2.026210\n",
      "iteration 1400 / 2000: loss 2.143447\n",
      "iteration 1500 / 2000: loss 2.100581\n",
      "iteration 1600 / 2000: loss 2.112973\n",
      "iteration 1700 / 2000: loss 2.131334\n",
      "iteration 1800 / 2000: loss 2.112545\n",
      "iteration 1900 / 2000: loss 2.072905\n",
      "training accuracy: 0.329245\n",
      "validation accuracy: 0.334000\n",
      "\n",
      " 4e-07 31250\n",
      "iteration 0 / 2000: loss 964.032986\n",
      "iteration 100 / 2000: loss 8.106837\n",
      "iteration 200 / 2000: loss 2.120078\n",
      "iteration 300 / 2000: loss 2.104323\n",
      "iteration 400 / 2000: loss 2.112770\n",
      "iteration 500 / 2000: loss 2.131498\n",
      "iteration 600 / 2000: loss 2.109042\n",
      "iteration 700 / 2000: loss 2.101923\n",
      "iteration 800 / 2000: loss 2.135697\n",
      "iteration 900 / 2000: loss 2.111025\n",
      "iteration 1000 / 2000: loss 2.145393\n",
      "iteration 1100 / 2000: loss 2.114076\n",
      "iteration 1200 / 2000: loss 2.067748\n",
      "iteration 1300 / 2000: loss 2.127112\n",
      "iteration 1400 / 2000: loss 2.120890\n",
      "iteration 1500 / 2000: loss 2.109077\n",
      "iteration 1600 / 2000: loss 2.130046\n",
      "iteration 1700 / 2000: loss 2.069457\n",
      "iteration 1800 / 2000: loss 2.119099\n",
      "iteration 1900 / 2000: loss 2.116125\n",
      "training accuracy: 0.310939\n",
      "validation accuracy: 0.335000\n",
      "\n",
      " 4e-07 37500\n",
      "iteration 0 / 2000: loss 1172.083662\n",
      "iteration 100 / 2000: loss 4.812929\n",
      "iteration 200 / 2000: loss 2.105148\n",
      "iteration 300 / 2000: loss 2.132160\n",
      "iteration 400 / 2000: loss 2.149621\n",
      "iteration 500 / 2000: loss 2.129946\n",
      "iteration 600 / 2000: loss 2.085923\n",
      "iteration 700 / 2000: loss 2.094850\n",
      "iteration 800 / 2000: loss 2.112203\n",
      "iteration 900 / 2000: loss 2.136932\n",
      "iteration 1000 / 2000: loss 2.115676\n",
      "iteration 1100 / 2000: loss 2.124525\n",
      "iteration 1200 / 2000: loss 2.098522\n",
      "iteration 1300 / 2000: loss 2.122483\n",
      "iteration 1400 / 2000: loss 2.127441\n",
      "iteration 1500 / 2000: loss 2.146920\n",
      "iteration 1600 / 2000: loss 2.124003\n",
      "iteration 1700 / 2000: loss 2.126219\n",
      "iteration 1800 / 2000: loss 2.096457\n",
      "iteration 1900 / 2000: loss 2.116640\n",
      "training accuracy: 0.324469\n",
      "validation accuracy: 0.339000\n",
      "\n",
      " 4e-07 43750\n",
      "iteration 0 / 2000: loss 1350.004990\n",
      "iteration 100 / 2000: loss 3.217639\n",
      "iteration 200 / 2000: loss 2.136988\n",
      "iteration 300 / 2000: loss 2.136726\n",
      "iteration 400 / 2000: loss 2.105561\n",
      "iteration 500 / 2000: loss 2.126062\n",
      "iteration 600 / 2000: loss 2.127119\n",
      "iteration 700 / 2000: loss 2.128358\n",
      "iteration 800 / 2000: loss 2.171478\n",
      "iteration 900 / 2000: loss 2.130041\n",
      "iteration 1000 / 2000: loss 2.127833\n",
      "iteration 1100 / 2000: loss 2.160891\n",
      "iteration 1200 / 2000: loss 2.147072\n",
      "iteration 1300 / 2000: loss 2.139006\n",
      "iteration 1400 / 2000: loss 2.137288\n",
      "iteration 1500 / 2000: loss 2.175304\n",
      "iteration 1600 / 2000: loss 2.165101\n",
      "iteration 1700 / 2000: loss 2.129026\n",
      "iteration 1800 / 2000: loss 2.144148\n",
      "iteration 1900 / 2000: loss 2.150660\n",
      "training accuracy: 0.293653\n",
      "validation accuracy: 0.314000\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.320449 val accuracy: 0.336000\n",
      "lr 1.000000e-07 reg 3.125000e+04 train accuracy: 0.323878 val accuracy: 0.336000\n",
      "lr 1.000000e-07 reg 3.750000e+04 train accuracy: 0.309878 val accuracy: 0.324000\n",
      "lr 1.000000e-07 reg 4.375000e+04 train accuracy: 0.316367 val accuracy: 0.322000\n",
      "lr 2.000000e-07 reg 2.500000e+04 train accuracy: 0.328592 val accuracy: 0.345000\n",
      "lr 2.000000e-07 reg 3.125000e+04 train accuracy: 0.324898 val accuracy: 0.340000\n",
      "lr 2.000000e-07 reg 3.750000e+04 train accuracy: 0.320694 val accuracy: 0.327000\n",
      "lr 2.000000e-07 reg 4.375000e+04 train accuracy: 0.299755 val accuracy: 0.321000\n",
      "lr 3.000000e-07 reg 2.500000e+04 train accuracy: 0.320776 val accuracy: 0.332000\n",
      "lr 3.000000e-07 reg 3.125000e+04 train accuracy: 0.320041 val accuracy: 0.334000\n",
      "lr 3.000000e-07 reg 3.750000e+04 train accuracy: 0.315122 val accuracy: 0.332000\n",
      "lr 3.000000e-07 reg 4.375000e+04 train accuracy: 0.308204 val accuracy: 0.311000\n",
      "lr 4.000000e-07 reg 2.500000e+04 train accuracy: 0.329245 val accuracy: 0.334000\n",
      "lr 4.000000e-07 reg 3.125000e+04 train accuracy: 0.310939 val accuracy: 0.335000\n",
      "lr 4.000000e-07 reg 3.750000e+04 train accuracy: 0.324469 val accuracy: 0.339000\n",
      "lr 4.000000e-07 reg 4.375000e+04 train accuracy: 0.293653 val accuracy: 0.314000\n",
      "best validation accuracy achieved during cross-validation: 0.345000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "n = 4\n",
    "for i in range(n):\n",
    "    l_rate = learning_rates[0] + i*(learning_rates[1] - learning_rates[0])/n\n",
    "    for j in range(n):\n",
    "        reg = int(regularization_strengths[0] + j*(regularization_strengths[1] - regularization_strengths[0])/n)\n",
    "        \n",
    "        print('\\n', l_rate, reg)    \n",
    "        clf = Softmax()\n",
    "        loss_hist = clf.train(X_train, y_train, learning_rate=l_rate, reg=reg, num_iters=2000, verbose=True)\n",
    "\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        acc_train = np.mean(y_train == y_train_pred)\n",
    "        print('training accuracy: %f' % acc_train)\n",
    "        y_val_pred = clf.predict(X_val)\n",
    "        acc_val = np.mean(y_val == y_val_pred)\n",
    "        print('validation accuracy: %f' % acc_val)\n",
    "\n",
    "        if acc_val > best_val:\n",
    "            best_val = acc_val\n",
    "            best_softmax = clf\n",
    "        \n",
    "        results.update({(l_rate,reg):(acc_train, acc_val)})\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.342000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:\n",
    "\n",
    "*Your explanation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
